from pyspark.sql import SparkSession
import pyarrow as pa
import pyarrow.parquet as pq

# Step 1: Initialize SparkSession
spark = SparkSession.builder \
    .appName("PyArrowTableFromSpark") \
    .enableHiveSupport() \  # Enable this if you're using Hive metastore
    .getOrCreate()

# Step 2: Specify existing and new table names
existing_table_name = "your_existing_table"  # Replace with the table name you want to read
new_table_name = "your_new_table"  # Replace with the table name you want to write

# Step 3: Load the existing table into a Spark DataFrame
spark_df = spark.table(existing_table_name)

# Step 4: Convert Spark DataFrame to Pandas DataFrame (if you need PyArrow Table)
pandas_df = spark_df.toPandas()

# Step 5: Convert Pandas DataFrame to PyArrow Table
arrow_table = pa.Table.from_pandas(pandas_df)

# Step 6: Save the PyArrow Table to a Parquet file (optional)
output_file = "output_table.parquet"
pq.write_table(arrow_table, output_file)

# Step 7: Write the Spark DataFrame to a new table
# Ensure you have permission to write to your metastore or database
spark_df.write.saveAsTable(new_table_name)

print(f"Data successfully read from {existing_table_name} and written to {new_table_name}")
