SELECT 
    t.TBL_NAME AS table_name,
    d.NAME AS database_name,
    t.OWNER AS owner
FROM 
    TBLS t
JOIN 
    DBS d ON t.DB_ID = d.DB_ID
WHERE 
    t.OWNER = '<user_name>';

==============================================

from pyspark.sql import SparkSession

# Create SparkSession
spark = SparkSession.builder \
    .appName("Hive Metastore Query") \
    .config("spark.sql.catalogImplementation", "hive") \
    .getOrCreate()

# Query the metastore to get tables owned by a specific user
user_name = "<owner_name>"
database_name = "<database_name>"

query = f"""
SELECT 
    t.tbl_name AS table_name, 
    d.name AS database_name, 
    t.owner AS owner
FROM 
    hive_metastore.tbls t
JOIN 
    hive_metastore.dbs d ON t.db_id = d.db_id
WHERE 
    t.owner = '{user_name}' 
    AND d.name = '{database_name}'
"""

# Run the query and display results
tables = spark.sql(query)
tables.show(truncate=False)



=====================================================================================
from pyspark.sql import SparkSession

# Create SparkSession with Hive support
spark = SparkSession.builder \
    .appName("Get Table Owners") \
    .enableHiveSupport() \
    .getOrCreate()

# Define the database
database_name = "<database_name>"  # Replace with your database name
spark.sql(f"USE {database_name}")

# Get a list of all tables in the database
tables = spark.sql("SHOW TABLES").collect()

# Iterate over tables and get owner information
print(f"Owner details for tables in database: {database_name}")
print("-------------------------------------------------")

for table in tables:
    table_name = table["tableName"]  # Extract table name from row
    formatted_info = spark.sql(f"DESCRIBE FORMATTED {table_name}").collect()
    
    # Find the owner field in the formatted output
    for row in formatted_info:
        if "Owner" in row["col_name"]:
            owner = row["data_type"]
            print(f"Table: {table_name}, Owner: {owner}")
            break


===========================================================
from pyspark.sql import SparkSession

# Create SparkSession with Hive support
spark = SparkSession.builder \
    .appName("Filter Tables by Owner") \
    .enableHiveSupport() \
    .getOrCreate()

# Define the database and owner
database_name = "your_database_name"  # Replace with your database name
specific_owner = "desired_owner_name"  # Replace with the specific owner's name

# Switch to the database
spark.sql(f"USE {database_name}")

# Get a list of all tables in the database
tables = spark.sql("SHOW TABLES").collect()

# Print header for the output
print(f"Tables owned by '{specific_owner}' in database: {database_name}")
print("-------------------------------------------------")

# Iterate over tables and filter by owner
for table in tables:
    table_name = table["tableName"]  # Extract table name from row
    formatted_info = spark.sql(f"DESCRIBE FORMATTED {table_name}").collect()
    
    # Find the owner field in the formatted output
    for row in formatted_info:
        if "Owner" in row["col_name"]:
            owner = row["data_type"]
            if owner == specific_owner:  # Check if the owner matches the desired owner
                print(f"Table: {table_name}, Owner: {owner}")
            break
